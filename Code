
## Import Modul
from google.colab import drive
import os
drive.mount("/content/drive/")
import numpy as np
import glob
Import pickle
import seaborn as sns
import pandas as pd
import pickle
import cv2
import tensorflow as tf
import plotly.express as px
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing import image
import matplotlib.pyplot as plt
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.applications import DenseNet201
from tensorflow.keras.applications.densenet import preprocess_input
from sklearn.metrics import classification_report
from itertools import product

## Pembagian Data 


# Load DataSet
file_path = "/content/drive/MyDrive/File fix/cotton1/cotton"
name_class = os.listdir(file_path)
name_class
filepaths = list(glob.glob(file_path+'/**/*.*'))

labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))
filepath = pd.Series(filepaths, name='Filepath').astype(str)
labels = pd.Series(labels, name='Label')
data = pd.concat([filepath, labels], axis=1)
data = data.sample(frac=1).reset_index(drop=True)

# Membagi data menjadi train, validasi, dan test (70:15:15)
train_size = 0.7
val_test_size = 0.15

data_train, data_temp = train_test_split(data, test_size=1-train_size, random_state=42, stratify=data['Label'])
data_val, data_test = train_test_split(data_temp, test_size=0.5, random_state=42, stratify=data_temp['Label'])

# Menampilkan jumlah data pada setiap set
print(f"Total data: {len(data)}")
print(f"Data latih: {len(data_train)}")
print(f"Data validasi: {len(data_val)}")
print(f"Data uji: {len(data_test)}")

# Simpan data dalam bentuk Excel
data_test.to_excel('/content/drive/MyDrive/File fix/data_test1.xlsx', index=True)
data_train.to_excel('/content/drive/MyDrive/File fix/data_train1.xlsx', index=True)
data_val.to_excel('/content/drive/MyDrive/File fix/data_val1.xlsx', index=True)

data_train = pd.read_excel('/content/drive/MyDrive/File fix/data_train1.xlsx', index_col=0)
data_val = pd.read_excel('/content/drive/MyDrive/File fix/data_val1.xlsx', index_col=0)

# Preprocessing data & Augmentasi (hanya data train)

train_data = ImageDataGenerator(
    preprocessing_function=preprocess_input,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

val_test_data = ImageDataGenerator(preprocessing_function=preprocess_input)

# Data Train
train = train_data.flow_from_dataframe(
    dataframe=data_train,
    x_col='Filepath',
    y_col='Label',
    target_size=(448, 448),
    class_mode='categorical',
    batch_size=32,
    shuffle=True,
    seed=42
)

# Data Validation
val = val_test_data.flow_from_dataframe(
    dataframe=data_val,
    x_col='Filepath',
    y_col='Label',
    target_size=(448, 448),
    class_mode='categorical',
    batch_size=32,
    shuffle=False
)


# Path
model_save_dir = "/content/drive/MyDrive/File fix/models/"
checkpoint_dir = "/content/drive/MyDrive/File fix/checkpoints/"
results_csv = "/content/drive/MyDrive/File fix/grid_search_results.csv"

# Buat folder jika belum ada
os.makedirs(model_save_dir, exist_ok=True)
os.makedirs(checkpoint_dir, exist_ok=True)

# Tentukan hyperparameter grid
hidden_units_list = [32, 64, 256, 512]
learning_rates = [1e-3, 1e-4, 1e-5]
dropouts = [0.4, 0.5, 0.6]
param_grid = list(product(hidden_units_list, learning_rates, dropouts))

# Periksa apakah file CSV hasil eksperimen ada untuk setiap hasil Hyperparameter
# jika file ada dan output menampilkan set kombinasi hyperparameter yang sudah dilatih maka  melatih set kombinasi hyperparameter selanjutnya (mencegah pelatihan ulang kombinasi yang sudah dievaluasi.)

if os.path.exists(results_csv):
    df_results = pd.read_csv(results_csv)
    trained_combinations = set(zip(df_results.hidden_units, df_results.learning_rate, df_results.dropout_rate))
else:
    df_results = pd.DataFrame(columns=['hidden_units', 'learning_rate', 'dropout_rate', 'val_accuracy', 'val_loss', 'model_path', 'history_path'])
    trained_combinations = set()

# Jumlah kelas
num_classes = len(train.class_indices)

# Mulai grid search
for hidden_units, lr, dropout_rate in param_grid:
    combo = (hidden_units, lr, dropout_rate)
    if combo in trained_combinations:
        print(f"â© Skip {combo} (already trained)")
        continue

    print(f"\nðŸ” Training: hidden_units={hidden_units}, lr={lr}, dropout={dropout_rate}")

    base_model = DenseNet201(
        input_shape=(448, 448, 3),
        include_top=False,
        weights='imagenet',
        pooling='avg'
    )
    base_model.trainable = False

    x = Dense(hidden_units, activation='relu')(base_model.output)
    x = Dropout(dropout_rate)(x)
    x = Dense(hidden_units, activation='relu')(x)
    x = Dropout(dropout_rate)(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=base_model.input, outputs=outputs)
    model.compile(optimizer=Adam(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])

    # Path
    model_filename = f"model_h{hidden_units}_lr{lr}_do{dropout_rate}.h5"
    model_path = os.path.join(model_save_dir, model_filename)
    history_path = model_path.replace('.h5', '_history.pkl')
    checkpoint_path = os.path.join(checkpoint_dir, f"checkpoint_h{hidden_units}_lr{lr}_do{dropout_rate}.weights.h5")

    # Callbacks
    early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)
    checkpoint = ModelCheckpoint(filepath=checkpoint_path, monitor='val_accuracy', save_best_only=True, save_weights_only=True, verbose=1)

    # Jika checkpoint tersedia, maka proses training dilanjutkan dari sana
    if os.path.exists(checkpoint_path):
        print(f"âœ… Loading checkpoint for {combo}")
        model.load_weights(checkpoint_path)

    # Training
    history = model.fit(
        train,
        validation_data=val,
        epochs=1000,
        batch_size=32,
        callbacks=[early_stopping, checkpoint],
        verbose=1
    )

    # Evaluasi
    val_loss, val_acc = model.evaluate(val, verbose=0)

    # Simpan model & history
    model.save(model_path)
    with open(history_path, 'wb') as f:
        pickle.dump(history.history, f)

    print(f"âœ… Saved model: {model_path}")
    print(f"ðŸ“ˆ Saved history: {history_path}")

    # Simpan hasil ke DataFrame dan CSV
    df_results.loc[len(df_results)] = {
        'hidden_units': hidden_units,
        'learning_rate': lr,
        'dropout_rate': dropout_rate,
        'val_accuracy': val_acc,
        'val_loss': val_loss,
        'model_path': model_path,
        'history_path': history_path
    }
    df_results.to_csv(results_csv, index=False)
    print(f"ðŸ“ Updated results: {results_csv}")

# Tampilkan hasil grid search yang sudah disorting berdasarkan val_accuracy
print("\nHasil Grid Search (disorting berdasarkan val_accuracy):")
print(df_results.sort_values(by='val_accuracy', ascending=False))

# Tampilkan file CSV Hasil Grid Search
results_csv = "/content/drive/MyDrive/File fix/grid_search_results.csv"
if os.path.exists(results_csv):
    df_results = pd.read_csv(results_csv)
else:
    print("Grid search results not found.")
    exit()

# Menampilkan Plot Accuracy dan Loss 
for index, row in df_results.iterrows():
    history_path = row['history_path']
    hidden_units = row['hidden_units']
    lr = row['learning_rate']
    dropout_rate = row['dropout_rate']

    if os.path.exists(history_path):
        with open(history_path, 'rb') as f:
            history = pickle.load(f)

        print(f"\nPlotting history for: hidden_units={hidden_units}, lr={lr}, dropout={dropout_rate}")

        # Plot training & validation accuracy values
        plt.figure(figsize=(12, 4))
        plt.subplot(1, 2, 1)
        plt.plot(history['accuracy'])
        plt.plot(history['val_accuracy'])
        plt.title(f'Model Accuracy (h={hidden_units}, lr={lr}, do={dropout_rate})')
        plt.ylabel('Accuracy')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Validation'], loc='upper left')

        # Plot training & validation loss values
        plt.subplot(1, 2, 2)
        plt.plot(history['loss'])
        plt.plot(history['val_loss'])
        plt.title(f'Model Loss (h={hidden_units}, lr={lr}, do={dropout_rate})')
        plt.ylabel('Loss')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Validation'], loc='upper left')

        plt.tight_layout()
        plt.show()
    else:
        print(f"History file not found for {history_path}")

## Buat Parallel Coordinates Plot dari Hasil Grid Search

# Pastikan file CSV ada sebelum mencoba membacanya
if os.path.exists(results_csv):
    # Muat hasil dari CSV ke DataFrame
    df_results = pd.read_csv(results_csv)

    # Pastikan kolom-kolom yang diperlukan
    required_columns = ['hidden_units', 'learning_rate', 'dropout_rate', 'val_accuracy', 'val_loss']
    if all(col in df_results.columns for col in required_columns):
        # Buat parallel coordinates plot
        fig = px.parallel_coordinates(
            df_results,
            dimensions=required_columns,
            title="Hasil Grid Search: Parallel Coordinates Plot",
            color="val_accuracy",  # Warna garis berdasarkan validasi akurasi
            color_continuous_scale=px.colors.sequential.Rainbow, # Skala warna
            labels={
                "hidden_units": "Jumlah Unit Tersembunyi",
                "learning_rate": "Learning Rate",
                "dropout_rate": "Tingkat Dropout",
                "val_accuracy": "Akurasi Validasi",
                "val_loss": "Loss Validasi",
            },
        )

        # Atur layout agar tampilan lebih baik
        fig.update_layout(
            plot_bgcolor="black",
            paper_bgcolor="white",
            font=dict(size=10),
            title_x=0.5, # Pusatkan judul
        )
        # Tampilkan plot
        fig.show()

# Uji model dengan data test
best_model_path = '/content/drive/MyDrive/File fix/models/model_h512_lr0.001_do0.5.h5'

data_test = pd.read_excel('/content/drive/MyDrive/File fix/data_test1.xlsx', index_col=0)

val_test_data = ImageDataGenerator(preprocessing_function=preprocess_input)

# Data Test
test = val_test_data.flow_from_dataframe(
    dataframe=data_test,
    x_col='Filepath',
    y_col='Label',
    target_size=(448, 448),
    class_mode='categorical',
    batch_size=32,
    shuffle=False
)

# Class labels sesuai urutan output model
class_labels = ['Aphids', 'Army Worm', 'Bacterial Blight', 'Healthy', 'Powdery Mildew', 'Target Spot']

# Load the saved model
loaded_model = tf.keras.models.load_model(best_model_path)

# Prediksi pada data tes
y_pred = loaded_model.predict(test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = test.classes # Label sebenarnya

# Classification Report
print("\nClassification Report:")
print(classification_report(y_true, y_pred_classes, target_names=class_labels))

# Hitung confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)

# Tampilkan confusion matrix
fig, ax = plt.subplots(figsize=(10, 10)) # Sesuaikan ukuran figure
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)
disp.plot(ax=ax, xticks_rotation='vertical') # Mengatur rotasi label sumbu x agar vertikal
plt.show()


# Lists untuk menyimpan label sebenarnya dan prediksi
true_labels = []
pred_labels = []

# Dictionary untuk menyimpan jumlah kesalahan prediksi per kelas
misclassifications = {label: 0 for label in class_labels}
misclassified_images = []  # Menyimpan daftar gambar yang diprediksi salah
correctly_classified_images = []  # Menyimpan daftar gambar yang diprediksi benar

for index, row in data_test.iterrows():
    image_path = row['Filepath']
    true_label = row['Label']

    img = load_img(image_path, target_size=(448, 448))
    x = img_to_array(img)
    x = preprocess_input(x)
    x = np.expand_dims(x, axis=0)

    pred_probs = loaded_model.predict(x, verbose=0)
    pred_class_idx = np.argmax(pred_probs, axis=-1)[0]
    predicted_label = class_labels[pred_class_idx]

    true_labels.append(true_label)
    pred_labels.append(predicted_label)

    print(f"File: {os.path.basename(image_path)} | True Label: {true_label} | Predicted Label: {predicted_label}")

    if predicted_label != true_label:
        misclassifications[true_label] += 1
        misclassified_images.append((image_path, true_label, predicted_label))
    else:
        correctly_classified_images.append((image_path, true_label))

# Tampilkan jumlah kesalahan prediksi per kelas
print("\nJumlah kesalahan prediksi per kelas:")
for label, count in misclassifications.items():
    print(f"{label}: {count}")

# Tampilkan Semua Hasil Prediksi Benar

num_images_per_class = 10  # Jumlah gambar yang ingin ditampilkan per kelas

for class_name in class_labels:
    class_images = [img_path for img_path, true_label in correctly_classified_images if true_label == class_name]

    if len(class_images) > 0:
        num_images_to_display = min(num_images_per_class, len(class_images))  # Batasi hingga 10 gambar atau jumlah gambar yang tersedia

        fig, axes = plt.subplots(1, num_images_to_display, figsize=(num_images_to_display * 3, 3))

        for i in range(num_images_to_display):
            img = load_img(class_images[i], target_size=(512, 512))
            axes[i].imshow(img)
            axes[i].axis('off')
            axes[i].set_title(f"True: {class_name}", fontsize=8)

        plt.tight_layout()
        plt.show()
    else:
        print(f"Tidak ada prediksi yang benar untuk kelas: {class_name}")


# Tampilkan Semua Hasil Prediksi Salah

total_images = len(misclassified_images)
if total_images > 0:
    cols = 5  # Jumlah kolom dalam grid
    rows = (total_images + cols - 1) // cols  # Hitung jumlah baris yang dibutuhkan
    fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))
    axes = axes.flatten()

    for i, (image_path, true_label, predicted_label) in enumerate(misclassified_images):
        img = load_img(image_path, target_size=(512, 512))
        axes[i].imshow(img)
        axes[i].axis('off')
        axes[i].set_title(f"True: {true_label}\nPred: {predicted_label}", fontsize=8)

    for j in range(i + 1, len(axes)):
        axes[j].axis('off')
    plt.tight_layout()
    plt.show()
else:
    print("Tidak ada kesalahan prediksi.")

# GRAD-CAM

# Nonaktifkan softmax di layer terakhir
loaded_model.layers[-1].activation = None

# Nama layer konvolusi terakhir
last_conv_layer_name = "conv5_block29_concat"  # Ganti sesuai arsitektur model Anda

# Grad-CAM: Membuat heatmap
def make_Grad-CAM_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
    grad_model = tf.keras.models.Model(
        [model.inputs],
        [model.get_layer(last_conv_layer_name).output, model.output]
    )

    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        class_output = predictions[:, pred_index]

    grads = tape.gradient(class_output, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = tf.maximum(heatmap, 0) / tf.reduce_max(heatmap + tf.keras.backend.epsilon())
    return heatmap.numpy()

# Gabungkan heatmap dengan gambar asli
def overlay_Grad-CAM(img_path, heatmap, alpha=0.5):
    img = image.load_img(img_path)
    img = image.img_to_array(img)

    heatmap = np.uint8(255 * heatmap)
    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
    heatmap_color = plt.cm.jet(heatmap)[:, :, :3] * 255

    superimposed_img = heatmap_color * alpha + img
    return tf.keras.utils.array_to_img(superimposed_img)

# Fungsi untuk memuat dan mempersiapkan gambar
def get_img_array(img_path, size=(448, 448)):
    img = image.load_img(img_path, target_size=size)
    arr = image.img_to_array(img)
    arr = np.expand_dims(arr, axis=0)
    return arr


# Terapkan Grad-CAM untuk Prediksi Benar
for class_name in class_labels:
    print(f"\nProcessing Grad-CAM for class: {class_name} (Correct Predictions)")

    correct_images = [img_path for img_path, label in correctly_classified_images if label == class_name]
    if len(correct_images) > 5:
        correct_images = random.sample(correct_images, 5)

    for image_path in correct_images:
        img_array = preprocess_input(get_img_array(image_path, size=(448, 448)))
        heatmap = make_Grad-CAM_heatmap(img_array, loaded_model, last_conv_layer_name)
        Grad-CAM_img = overlay_Grad-CAM(image_path, heatmap)

        orig_img = image.load_img(image_path)
        fig, ax = plt.subplots(1, 3, figsize=(12, 4))

        ax[0].imshow(orig_img)
        ax[0].set_title(f"Input Image\n(True: {class_name})")
        ax[0].axis('off')

        ax[1].imshow(orig_img)
        ax[1].imshow(heatmap, alpha=0.8, cmap='jet')
        ax[1].set_title("Heatmap")
        ax[1].axis('off')

        ax[2].imshow(Grad-CAM_img)
        ax[2].set_title(f"Grad-CAM\n(Predicted: {class_name})")
        ax[2].axis('off')

        plt.show()

# Terapkan Grad-Cam ke dalam Gambar (Prediksi Salah)
print("\nProcessing Grad-CAM for Incorrect Predictions")

for image_path, true_label, predicted_label in misclassified_images:
    # Memuat dan lakukan preprocessing terhadap gambar
    img_array = preprocess_input(get_img_array(image_path, size=(448, 448)))

    # Generate Grad-CAM heatmap
    heatmap = make_Grad-CAM_heatmap(img_array, loaded_model, last_conv_layer_name)

    # Overlay heatmap pada gambar
    Grad-CAM_img = overlay_Grad-CAM(image_path, heatmap)

    # Tampilkan Gambar
    orig_img = load_img(image_path)
    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))

    ax[0].imshow(orig_img)
    ax[0].set_title(f"Input Image\n(True: {true_label})")
    ax[0].axis('off')

    ax[1].imshow(orig_img)
    ax[1].imshow(heatmap, alpha=0.8, cmap="jet")
    ax[1].set_title("Heatmap")
    ax[1].axis('off')

    ax[2].imshow(Grad-CAM_img)
    ax[2].set_title(f"Grad-CAM\n(Predicted: {predicted_label})")
    ax[2].axis('off')

    plt.show()




